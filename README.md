# Reading List

## Question Answering

| Title | Description |
| --- | --- |
| [What do Models Learn from Question Answering Datasets?](https://arxiv.org/abs/2004.03490) | Shows that models trained on extractive question-answering datasets tend to have biaises and lack generalization to out-of-domain. Models seems to be able to answer without the question in some extent (similar to MCQA). |

## Unsupervised Question Answering

| Title | Description |
| --- | --- |
| [Unsupervised Multi-hop Question Answering by Question Generation](https://arxiv.org/abs/2010.12623) | Uses multiple operators to extract and merge information from multiple contexts. |
| [Training Question Answering Models From Synthetic Data](https://arxiv.org/abs/2002.09599) | A data augmentation method using pretrained models and the SQUAD training set to select answers and questions from GPT-2 generated data. |

## BERT and Pretrained Models

| Title | Description |
| --- | --- |
| [Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment](https://arxiv.org/abs/1907.11932) | Describes a two-step attack method to change the prediction of a model (e.g. BERT). First find the most important words by looking at the effect of removing it then replace them by semantically similar substitutes until the prediction changes. |
| [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341) | Study of the attention layers inside BERT and how well they corelate to syntactic features of the sentences. |

## Others

| Title | Description |
| --- | --- |
